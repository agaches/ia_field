# Ollama + Otoroshi - Configuration LLM avec API Gateway

Ce projet configure une infrastructure LLM locale avec :
- **Ollama** : Serveur d'infÃ©rence LLM avec support GPU AMD (ROCm)
- **Otoroshi** : API Gateway pour sÃ©curiser et gÃ©rer l'accÃ¨s Ã  Ollama

## ğŸš€ DÃ©marrage rapide

```bash
# 1. DÃ©marrer l'infrastructure
docker-compose up -d

# 2. Attendre le dÃ©marrage (15 secondes)
sleep 15

# 3. Configurer Otoroshi (crÃ©er la route et l'API key)
./setup_otoroshi.sh

# 4. Charger un modÃ¨le
docker exec ollama ollama pull llama2

# 5. Tester
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     http://localhost:8080/v1/models
```

## ğŸ“‹ PrÃ©requis

- Docker et Docker Compose
- GPU AMD compatible avec ROCm (optionnel, sinon utiliser `ollama/ollama:latest`)
- Permissions Docker : `sudo usermod -aG docker $USER`

## ğŸ”§ Configuration

### Credentials par dÃ©faut

**Otoroshi Admin UI** (http://localhost:8080)
- Login: `admin@otoroshi.io`
- Password: `password`

**API Key pour accÃ©der Ã  Ollama**
- Client ID: `my-llm-client-id`
- Client Secret: `my-llm-client-secret`

### Ports exposÃ©s

- `8080` : Otoroshi (UI + API)
- `8443` : Otoroshi HTTPS
- `11434` : Ollama (accÃ¨s direct, optionnel)

## ğŸ“š Exemples d'utilisation

### Lister les modÃ¨les disponibles

```bash
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     http://localhost:8080/v1/models
```

### Chat Completion

```bash
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     -H 'Content-Type: application/json' \
     -d '{
       "model": "llama2",
       "messages": [
         {"role": "user", "content": "Hello!"}
       ]
     }' \
     http://localhost:8080/v1/chat/completions
```

### Streaming Response

```bash
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     -H 'Content-Type: application/json' \
     -d '{
       "model": "llama2",
       "messages": [
         {"role": "user", "content": "Write a short poem"}
       ],
       "stream": true
     }' \
     http://localhost:8080/v1/chat/completions
```

## ğŸ”„ Gestion des modÃ¨les

```bash
# TÃ©lÃ©charger un modÃ¨le
docker exec ollama ollama pull llama2
docker exec ollama ollama pull mistral
docker exec ollama ollama pull codellama

# Lister les modÃ¨les locaux
docker exec ollama ollama list

# Supprimer un modÃ¨le
docker exec ollama ollama rm llama2
```

## ğŸ› ï¸ Maintenance

### RedÃ©marrer l'infrastructure

```bash
docker-compose restart
```

### RedÃ©marrer complÃ¨tement (avec nettoyage)

```bash
docker-compose down -v
docker-compose up -d
sleep 15
./setup_otoroshi.sh
```

### Voir les logs

```bash
# Logs Otoroshi
docker logs otoroshi -f

# Logs Ollama
docker logs ollama -f

# Logs des deux services
docker-compose logs -f
```

## ğŸ“Š Monitoring et Benchmarks

### Monitoring des ressources

```bash
# Snapshot unique des ressources
./monitor_resources.sh

# Monitoring continu avec GPU (si mode GPU)
./monitor_resources.sh continuous

# Export des statistiques
./monitor_resources.sh export
```

**Note:** Le monitoring GPU affichera un avertissement en mode CPU ou si le GPU n'est pas supportÃ©.

### Benchmarks de performance

```bash
# Rendre les scripts exÃ©cutables
chmod +x monitor_resources.sh benchmark.sh

# Test de latence (10 requÃªtes sÃ©quentielles)
./benchmark.sh latency 10

# Test de charge (5 requÃªtes simultanÃ©es pendant 30s)
./benchmark.sh load 5 30

# Benchmark complet
./benchmark.sh full
```

### MÃ©triques surveillÃ©es

- **CPU**: Utilisation par conteneur et globale
- **MÃ©moire**: RAM utilisÃ©e par conteneur et systÃ¨me
- **GPU**: Utilisation GPU AMD (si ROCm installÃ©)
- **RÃ©seau**: I/O rÃ©seau par conteneur
- **Disque**: Utilisation des volumes Docker
- **Latence**: Temps de rÃ©ponse des requÃªtes
- **Throughput**: RequÃªtes par seconde

## ğŸ”’ SÃ©curitÃ©

### Deux types de credentials

1. **Credentials ADMIN** (pour gÃ©rer Otoroshi)
   - Client ID: `admin-api-apikey-id`
   - Client Secret: `admin-api-apikey-secret`
   - Usage: CrÃ©er routes, API keys, configuration via API d'admin
   - âš ï¸ Ne jamais utiliser pour les requÃªtes applicatives

2. **API Key CLIENT** (pour accÃ©der Ã  Ollama)
   - Client ID: `my-llm-client-id`
   - Client Secret: `my-llm-client-secret`
   - Usage: AccÃ¨s Ã  Ollama via Otoroshi
   - âœ… Ã€ utiliser dans vos applications

### Test de sÃ©curitÃ©

Lancez le script de test pour vÃ©rifier :
```bash
chmod +x test_api.sh
./test_api.sh
```

- L'accÃ¨s Ã  Ollama est protÃ©gÃ© par API key via Otoroshi
- Les quotas sont configurÃ©s (10M requÃªtes/jour)
- Pour la production, changez les credentials par dÃ©faut
- Configurez HTTPS via Otoroshi pour les connexions externes

## ğŸ“– Documentation

- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Otoroshi Documentation](https://maif.github.io/otoroshi/manual/)
- [OpenAI API Compatibility](https://github.com/ollama/ollama/blob/main/docs/openai.md)

## âš¡ GPU AMD (ROCm)

### âš ï¸ Important: GPU Radeon 860M (RDNA3.5)

Votre GPU **AMD Radeon 860M** (intÃ©grÃ© au Ryzen AI 7 350) est trÃ¨s rÃ©cent (2024) et le **support ROCm est encore expÃ©rimental**.

**Deux modes disponibles :**

### Mode CPU (RecommandÃ© - Stable)

```bash
chmod +x switch_mode.sh
./switch_mode.sh cpu
```

**Avantages:**
- âœ… Stable et fonctionnel
- âœ… Bonne performance avec votre Ryzen AI 7 350 (8 cÅ“urs)
- âœ… Pas de problÃ¨mes de compatibilitÃ©

**InconvÃ©nients:**
- âŒ Plus lent que GPU pour les grands modÃ¨les
- âŒ Consommation mÃ©moire RAM plus importante

### Mode GPU (ExpÃ©rimental)

```bash
./switch_mode.sh gpu
docker logs ollama -f  # Surveillez les logs
```

**Note:** Le Radeon 860M (Device ID: 1114) n'est pas encore officiellement supportÃ© par ROCm.
Vous pouvez essayer mais des erreurs sont attendues.

### VÃ©rifier le mode actuel

```bash
./switch_mode.sh status
```

### Configuration GPU requise (si mode GPU)

**1. Ajouter votre utilisateur au groupe video :**

```bash
sudo usermod -aG video $USER
# DÃ©connexion/reconnexion nÃ©cessaire
```

**2. Identifier la version GFX de votre GPU :**

```bash
# Votre GPU Radeon 860M est probablement:
# - gfx1102 (RDNA3.5) â†’ HSA_OVERRIDE_GFX_VERSION=11.0.2
# - ou gfx1100 (RDNA3) â†’ HSA_OVERRIDE_GFX_VERSION=11.0.0

rocminfo | grep gfx
```

**3. Ajuster docker-compose.yml si nÃ©cessaire**

### Performances comparatives

```bash
# Benchmark CPU vs GPU
./benchmark.sh latency 5
```

**Attendu avec votre configuration:**
- **CPU Mode**: ~10-30 tokens/sec (selon le modÃ¨le)
- **GPU Mode**: Si compatible, ~30-100 tokens/sec

## ğŸ¯ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP + API Key
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Otoroshi      â”‚ (Port 8080)
â”‚   API Gateway   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP (rÃ©seau interne)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚ (Port 11434)
â”‚   LLM Server    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
