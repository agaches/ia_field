# Ollama + Otoroshi - Configuration LLM avec API Gateway

Ce projet configure une infrastructure LLM locale avec :
- **Ollama** : Serveur d'infÃ©rence LLM avec support GPU AMD (ROCm)
- **Otoroshi** : API Gateway pour sÃ©curiser et gÃ©rer l'accÃ¨s Ã  Ollama

## ğŸš€ DÃ©marrage rapide

```bash
# 1. DÃ©marrer l'infrastructure
docker-compose up -d

# 2. Attendre le dÃ©marrage (15 secondes)
sleep 15

# 3. Configurer Otoroshi (crÃ©er la route et l'API key)
./setup_otoroshi.sh

# 4. Charger un modÃ¨le
docker exec ollama ollama pull llama2

# 5. Tester
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     http://localhost:8080/v1/models
```

## ğŸ“‹ PrÃ©requis

- Docker et Docker Compose
- GPU AMD compatible avec ROCm (optionnel, sinon utiliser `ollama/ollama:latest`)
- Permissions Docker : `sudo usermod -aG docker $USER`

## ğŸ”§ Configuration

### Credentials par dÃ©faut

**Otoroshi Admin UI** (http://localhost:8080)
- Login: `admin@otoroshi.io`
- Password: `password`

**API Key pour accÃ©der Ã  Ollama**
- Client ID: `my-llm-client-id`
- Client Secret: `my-llm-client-secret`

### Ports exposÃ©s

- `8080` : Otoroshi (UI + API)
- `8443` : Otoroshi HTTPS
- `11434` : Ollama (accÃ¨s direct, optionnel)

## ğŸ“š Exemples d'utilisation

### Lister les modÃ¨les disponibles

```bash
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     http://localhost:8080/v1/models
```

### Chat Completion

```bash
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     -H 'Content-Type: application/json' \
     -d '{
       "model": "llama2",
       "messages": [
         {"role": "user", "content": "Hello!"}
       ]
     }' \
     http://localhost:8080/v1/chat/completions
```

### Streaming Response

```bash
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     -H 'Content-Type: application/json' \
     -d '{
       "model": "llama2",
       "messages": [
         {"role": "user", "content": "Write a short poem"}
       ],
       "stream": true
     }' \
     http://localhost:8080/v1/chat/completions
```

## ğŸ”„ Gestion des modÃ¨les

```bash
# TÃ©lÃ©charger un modÃ¨le
docker exec ollama ollama pull llama2
docker exec ollama ollama pull mistral
docker exec ollama ollama pull codellama

# Lister les modÃ¨les locaux
docker exec ollama ollama list

# Supprimer un modÃ¨le
docker exec ollama ollama rm llama2
```

## ğŸ› ï¸ Maintenance

### RedÃ©marrer l'infrastructure

```bash
docker-compose restart
```

### RedÃ©marrer complÃ¨tement (avec nettoyage)

```bash
docker-compose down -v
docker-compose up -d
sleep 15
./setup_otoroshi.sh
```

### Voir les logs

```bash
# Logs Otoroshi
docker logs otoroshi -f

# Logs Ollama
docker logs ollama -f

# Logs des deux services
docker-compose logs -f
```

## ğŸ”’ SÃ©curitÃ©

- L'accÃ¨s Ã  Ollama est protÃ©gÃ© par API key via Otoroshi
- Les quotas sont configurÃ©s (10M requÃªtes/jour)
- Pour la production, changez les credentials par dÃ©faut
- Configurez HTTPS via Otoroshi pour les connexions externes

## ğŸ“– Documentation

- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Otoroshi Documentation](https://maif.github.io/otoroshi/manual/)
- [OpenAI API Compatibility](https://github.com/ollama/ollama/blob/main/docs/openai.md)

## âš¡ GPU AMD (ROCm)

Le conteneur utilise l'image `ollama/ollama:rocm` pour le support GPU AMD.
Les devices `/dev/kfd` et `/dev/dri` sont mappÃ©s automatiquement.

Pour utiliser CPU uniquement, modifiez `docker-compose.yml` :
```yaml
ollama:
  image: ollama/ollama:latest  # Au lieu de :rocm
  # Supprimez la section devices
```

## ğŸ¯ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP + API Key
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Otoroshi      â”‚ (Port 8080)
â”‚   API Gateway   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP (rÃ©seau interne)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚ (Port 11434)
â”‚   LLM Server    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
