# Ollama + Otoroshi - LLM Local avec API Gateway

## ğŸ“ Contexte

Infrastructure locale pour hÃ©berger des modÃ¨les LLM avec sÃ©curisation via API Gateway.
- **Ollama** : Serveur d'infÃ©rence LLM (mode CPU ou GPU AMD)
- **Otoroshi** : API Gateway pour authentification et gestion des accÃ¨s
- **DÃ©ploiement** : Docker Compose, configuration automatisÃ©e

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚  RequÃªtes avec API Key
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Otoroshi      â”‚  Port 8080 - API Gateway
â”‚                 â”‚  - Authentification (API Keys)
â”‚                 â”‚  - Routage vers ollama.oto.tools
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚  Port 11434 - Serveur LLM
â”‚                 â”‚  - Support CPU/GPU AMD (ROCm)
â”‚                 â”‚  - Compatible API OpenAI
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ports exposÃ©s :**
- `8080` : Otoroshi (UI Admin + API Gateway)
- `8443` : Otoroshi HTTPS
- `11434` : Ollama (accÃ¨s direct interne)

## ğŸš€ Commandes Essentielles

### Configuration initiale (une seule fois)

Configurez votre fichier `/etc/hosts` pour accÃ©der aux services via leurs noms de domaine :

```bash
# Ajouter les entrÃ©es DNS locales
echo "127.0.0.1 otoroshi.oto.tools otoroshi-api.oto.tools ollama.oto.tools" | sudo tee -a /etc/hosts
```

**AccÃ¨s Ã  l'interface Admin Otoroshi :**
- ğŸŒ URL : http://otoroshi.oto.tools:8080
- ğŸ‘¤ Login : `admin@otoroshi.io`
- ğŸ”‘ Mot de passe : `password`

### DÃ©marrer l'infrastructure

```bash
# 1. Lancer les conteneurs
docker-compose up -d

# 2. Attendre le dÃ©marrage (15 secondes)
sleep 15

# 3. Configurer Otoroshi automatiquement
./setup/setup_otoroshi.sh

# 4. Charger un modÃ¨le
docker exec ollama ollama pull phi

# 5. Tester l'accÃ¨s
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     http://localhost:8080/v1/models
```

### GÃ©rer les modÃ¨les

```bash
# TÃ©lÃ©charger un modÃ¨le
docker exec ollama ollama pull llama2
docker exec ollama ollama pull mistral

# Lister les modÃ¨les installÃ©s
docker exec ollama ollama list

# Supprimer un modÃ¨le
docker exec ollama ollama rm llama2

# Changer de modÃ¨le : il suffit de le charger puis de l'utiliser dans vos requÃªtes
docker exec ollama ollama pull codellama
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     -H 'Content-Type: application/json' \
     -d '{"model":"codellama","messages":[{"role":"user","content":"Hello"}]}' \
     http://localhost:8080/v1/chat/completions
```

### GÃ©rer l'infrastructure

```bash
# RedÃ©marrer
docker-compose restart

# ArrÃªter
docker-compose down

# Nettoyage complet (supprime les donnÃ©es)
docker-compose down -v

# Voir les logs
docker logs ollama -f
docker logs otoroshi -f
```

### CrÃ©er un client/clÃ© API dans Otoroshi

**Via l'interface Admin :**
1. AccÃ©der Ã  http://localhost:8080
2. Login : `admin@otoroshi.io` / Password : `password`
3. Menu **Settings** â†’ **Apikeys**
4. Cliquer **Create new apikey**
5. Remplir :
   - Name : `mon-app-client`
   - Client ID : `mon-client-id`
   - Client Secret : `mon-secret`
   - Authorized On : SÃ©lectionner la route `ollama-route`
6. Sauvegarder

**Via API :**

```bash
curl -X POST http://localhost:8080/api/apikeys \
  -H 'Otoroshi-Client-Id: admin-api-apikey-id' \
  -H 'Otoroshi-Client-Secret: admin-api-apikey-secret' \
  -H 'Content-Type: application/json' \
  -d '{
    "clientId": "nouveau-client-id",
    "clientSecret": "nouveau-secret",
    "clientName": "Mon Application",
    "authorizedEntities": ["route_ollama-route"],
    "throttlingQuota": 10000000,
    "dailyQuota": 10000000
  }'
```

### Ajouter/changer de modÃ¨le

```bash
# Ajouter un nouveau modÃ¨le
docker exec ollama ollama pull mistral:7b

# Utiliser ce modÃ¨le dans vos requÃªtes
curl -H 'Otoroshi-Client-Id: my-llm-client-id' \
     -H 'Otoroshi-Client-Secret: my-llm-client-secret' \
     -H 'Host: ollama.oto.tools' \
     -H 'Content-Type: application/json' \
     -d '{"model":"mistral:7b","messages":[{"role":"user","content":"Bonjour"}]}' \
     http://localhost:8080/v1/chat/completions

# Changer pour un autre modÃ¨le dÃ©jÃ  chargÃ© : modifier le paramÃ¨tre "model" dans la requÃªte
```

### Basculer entre mode CPU et GPU

```bash
# Passer en mode CPU (recommandÃ©, stable)
./setup/switch_mode.sh cpu

# Passer en mode GPU (expÃ©rimental, Radeon 860M pas officiellement supportÃ©)
./setup/switch_mode.sh gpu

# VÃ©rifier le mode actuel
./setup/switch_mode.sh status
```

## ğŸ¯ AccÃ©der Ã  l'Admin Otoroshi

**URL :** http://localhost:8080

**Credentials Admin :**
- Email : `admin@otoroshi.io`
- Mot de passe : `password`

**Interface :**
- **Dashboard** : MÃ©triques et aperÃ§u
- **Routes** : GÃ©rer les routes (dont `ollama-route`)
- **Apikeys** : CrÃ©er/gÃ©rer les clÃ©s d'accÃ¨s clients
- **Settings** : Configuration globale

**âš ï¸ SÃ©curitÃ© :** En production, changez immÃ©diatement les credentials par dÃ©faut.

## ğŸ“Š Tests et Monitoring

```bash
# Tester l'API
./test/test_api.sh

# VÃ©rifier le GPU
./test/check_gpu.sh

# Monitorer les ressources
./test/monitor_resources.sh

# Benchmark de performance
./test/benchmark.sh latency 10
```

## ğŸ“ Structure du Projet

```
local_llm_otoroshi/
â”œâ”€â”€ docker-compose.yml          # Configuration des services
â”œâ”€â”€ .env                        # Variables d'environnement
â”œâ”€â”€ setup/                      # Scripts de configuration
â”‚   â”œâ”€â”€ setup_otoroshi.sh      # Config automatique Otoroshi
â”‚   â”œâ”€â”€ switch_mode.sh         # Basculer CPU/GPU
â”‚   â””â”€â”€ fix_gpu_permissions.sh # Permissions GPU
â””â”€â”€ test/                       # Scripts de test
    â”œâ”€â”€ test_api.sh            # Tests API
    â”œâ”€â”€ benchmark.sh           # Performance
    â”œâ”€â”€ monitor_resources.sh   # Monitoring
    â”œâ”€â”€ check_gpu.sh           # Diagnostic GPU
    â””â”€â”€ test_gpu_inference.sh  # Test infÃ©rence GPU
```

## ğŸ”’ Credentials

**Admin Otoroshi (gestion du gateway) :**
- Client ID : `admin-api-apikey-id`
- Client Secret : `admin-api-apikey-secret`
- âš ï¸ Ne pas utiliser pour les requÃªtes applicatives

**API Key Client (accÃ¨s Ollama) :**
- Client ID : `my-llm-client-id`
- Client Secret : `my-llm-client-secret`
- âœ… Ã€ utiliser dans vos applications

## ğŸ“– RÃ©fÃ©rences

- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Otoroshi Documentation](https://maif.github.io/otoroshi/manual/)
- [OpenAI API Compatibility](https://github.com/ollama/ollama/blob/main/docs/openai.md)
